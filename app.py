import joblib
import pandas as pd
import tldextract
from collections import Counter
import numpy as np
from urllib.parse import urlparse
from flask import Flask, render_template, request
from sklearn.preprocessing import LabelEncoder

app = Flask(__name__)

# Load the trained model
model = joblib.load(open('important/best_model.pkl', 'rb'))
print(type(model))
df = pd.read_csv('datasets/finaldata.csv')

domain_encoder = LabelEncoder().fit(df['domain'])
subdomain_encoder = LabelEncoder().fit(df['subdomain'])
suffix_encoder = LabelEncoder().fit(df['suffix'])


def extract_lexical_features(url):
    features = {}
    features['url_length'] = len(url)
    features['num_dots'] = url.count('.')
    features['num_slashes'] = url.count('/')
    features['num_hyphens'] = url.count('-')
    features['num_underscores'] = url.count('_')
    features['num_question_marks'] = url.count('?')
    features['num_ampersands'] = url.count('&')
    features['num_equals'] = url.count('=')
    features['num_at_symbols'] = url.count('@')
    features['num_hashes'] = url.count('#')
    features['num_percent_encodings'] = url.count('%')
    features['num_tildes'] = url.count('~')
    features['num_pluses'] = url.count('+')
    features['num_dollars'] = url.count('$')
    features['num_exclamations'] = url.count('!')
    features['num_commas'] = url.count(',')
    features['num_apostrophes'] = url.count("'")
    features['num_parentheses'] = url.count('(') + url.count(')')
    suspicious_words = ['login', 'secure', 'update', 'bank', 'free', 'win']
    features['suspicious_word_count'] = sum([1 for word in suspicious_words if word in url.lower()])
    extracted = tldextract.extract(url)
    features['domain'] = extracted.domain
    features['subdomain'] = extracted.subdomain
    features['suffix'] = extracted.suffix
    features['uses_https'] = 1 if url.startswith('https') else 0
    
    def calculate_entropy(s):
        p, lns = Counter(s), float(len(s))
        return -sum(count/lns * np.log2(count/lns) for count in p.values())
    
    features['entropy'] = calculate_entropy(url)
    features['num_digits'] = sum(c.isdigit() for c in url)
    features['num_special_chars'] = sum(not c.isalnum() for c in url)
    features['domain_length'] = len(extracted.domain)
    features['tld_length'] = len(extracted.suffix)
    features['num_path_segments'] = len(urlparse(url).path.split('/'))
    features['num_query_params'] = len(urlparse(url).query.split('&')) if urlparse(url).query else 0
    features['query_length'] = len(urlparse(url).query)
    features['first_path_segment_length'] = len(urlparse(url).path.split('/')[1]) if len(urlparse(url).path.split('/')) > 1 else 0
    features['proportion_numeric'] = sum(c.isdigit() for c in url) / len(url)
    features['proportion_alphabetic'] = sum(c.isalpha() for c in url) / len(url)
    features['proportion_special'] = sum(not c.isalnum() for c in url) / len(url)
    
    return features

def preprocess_url(url):
    features = extract_lexical_features(url)
    domain_value = domain_encoder.transform([features['domain']])[0] if features['domain'] in domain_encoder.classes_ else 1
    subdomain_value = subdomain_encoder.transform([features['subdomain']])[0] if features['subdomain'] in subdomain_encoder.classes_ else 1
    suffix_value = suffix_encoder.transform([features['suffix']])[0] if features['suffix'] in suffix_encoder.classes_ else 1
    feature_vector = [      
        features['url_length'],
        features['num_dots'],
        features['num_slashes'],
        features['num_hyphens'],
        features['num_underscores'],
        features['num_question_marks'],
        features['num_ampersands'],
        features['num_equals'],
        features['num_at_symbols'],
        features['num_hashes'],
        features['num_percent_encodings'],
        features['num_tildes'],
        features['num_pluses'],
        features['num_dollars'],
        features['num_exclamations'],
        features['num_commas'],
        features['num_apostrophes'],
        features['num_parentheses'],
        features['suspicious_word_count'],
        domain_value,        # Encoded domain
        subdomain_value,     # Encoded subdomain
        suffix_value,      # Encoded suffix
        features['tld_length'],
        features['domain_length'],
        features['num_path_segments'],
        features['num_query_params'],
        features['query_length'],
        features['first_path_segment_length'],
        features['proportion_numeric'],
        features['proportion_alphabetic'],
        features['proportion_special'],
        features['uses_https'],        
        features['entropy'],          
        features['num_digits'],        
        features['num_special_chars'] 
    ]
    return feature_vector

    
@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == 'POST':
        url = request.form['url']
        # Process the URL and make a prediction
        result = predict_url(url)
        return render_template('result.html', result=result)
    return render_template('index.html')

def predict_url(url):   
    features = preprocess_url(url)
    probabilities = model.predict_proba([features])
    print(f"Probabilities for {url}: {probabilities}")  # Debugging step
    
    # Manually set threshold (adjust as necessary)
    malicious_threshold = 0.8  # Try lowering it back to default 0.5
    
    # Use the max predicted class
    predicted_class = np.argmax(probabilities)
    print(f"Predicted class: {predicted_class}")
    
    if probabilities[0][predicted_class] < malicious_threshold and predicted_class == 1:
        return 'Malicious'
    else:
        return 'Benign'
    
features = preprocess_url("www.google.com")
print("Extracted features for www.google.com:", features)



    
    # features = preprocess_url(url)
    # print("Extracted Features: ", features)  # Debug step
    
    # probabilities = model.predict_proba([features])
    # print("Prediction Probabilities: ", probabilities)  # Debug step
    
    # if probabilities[0][1] > 0.7:  # Adjust threshold from 0.5 to 0.8
    #     return 'Malicious'
    # else:
    #     return 'Benign'



@app.route('/how-it-works')
def how_it_works():
    return render_template('how_it_works.html')

if __name__ == '__main__':
    app.run(debug=True)